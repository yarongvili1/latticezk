#include <limits.h>
#include <vector>
#include <iostream>
#include <chrono>
#include "cuda.h"
#include "curand.h"

// This should really be a standard CURAND function
static const char* curandGetErrorString(curandStatus_t error)
{
	switch (error)
	{
	case CURAND_STATUS_SUCCESS:
		return "CURAND_STATUS_SUCCESS";

	case CURAND_STATUS_VERSION_MISMATCH:
		return "CURAND_STATUS_VERSION_MISMATCH";

	case CURAND_STATUS_NOT_INITIALIZED:
		return "CURAND_STATUS_NOT_INITIALIZED";

	case CURAND_STATUS_ALLOCATION_FAILED:
		return "CURAND_STATUS_ALLOCATION_FAILED";

	case CURAND_STATUS_TYPE_ERROR:
		return "CURAND_STATUS_TYPE_ERROR";

	case CURAND_STATUS_OUT_OF_RANGE:
		return "CURAND_STATUS_OUT_OF_RANGE";

	case CURAND_STATUS_LENGTH_NOT_MULTIPLE:
		return "CURAND_STATUS_LENGTH_NOT_MULTIPLE";

	case CURAND_STATUS_DOUBLE_PRECISION_REQUIRED:
		return "CURAND_STATUS_DOUBLE_PRECISION_REQUIRED";

	case CURAND_STATUS_LAUNCH_FAILURE:
		return "CURAND_STATUS_LAUNCH_FAILURE";

	case CURAND_STATUS_PREEXISTING_FAILURE:
		return "CURAND_STATUS_PREEXISTING_FAILURE";

	case CURAND_STATUS_INITIALIZATION_FAILED:
		return "CURAND_STATUS_INITIALIZATION_FAILED";

	case CURAND_STATUS_ARCH_MISMATCH:
		return "CURAND_STATUS_ARCH_MISMATCH";

	case CURAND_STATUS_INTERNAL_ERROR:
		return "CURAND_STATUS_INTERNAL_ERROR";
	}

	return "<unknown>";
}

namespace LatticeZK {
	namespace Cuda {

class BytesSampler;

__device__ inline unsigned int cuda_tid()
{
	unsigned int x = threadIdx.x + blockIdx.x * blockDim.x;
	unsigned int y = threadIdx.y + blockIdx.y * blockDim.y;
	unsigned int z = threadIdx.z + blockIdx.z * blockDim.z;
	unsigned int tid = x + blockDim.x * gridDim.x * (y + z * blockDim.y * gridDim.y);
	return tid;
}

	} // namespace Cuda
} // namespace LatticeZK

// Include Facct code in GPU mode
#define LATTICEZK_GAUSSIAN_FACCT_CUDA
#include "latticezk/gaussian/facct.inl"
#undef LATTICEZK_GAUSSIAN_FACCT_CUDA

#define CUDA_ALERT(expr, msg) do { auto _status = (expr); if (_status != cudaSuccess) printf("cuda error : %s : %s\n", cudaGetErrorString(_status), msg); } while (0);
#define CURAND_ALERT(expr, msg) do { auto _status = (expr); if (_status != CURAND_STATUS_SUCCESS) printf("curand error : %s : %s\n", curandGetErrorString(_status), msg); } while (0);

namespace LatticeZK {
	namespace Cuda {

constexpr size_t FACCT_SAMPLE_BLOCK_SIZE = 8; // number of Facct samples co-generated


// Used for allocating and auto-deallocating of the samples buffer
class SamplesSetup
{
private:
	const size_t num_blocks;
	int64_t * d_samples;
public:
	SamplesSetup(size_t num_blocks) :
		num_blocks(num_blocks)
	{
		CUDA_ALERT(cudaMalloc(&d_samples, FACCT_SAMPLE_BLOCK_SIZE * num_blocks * sizeof(int64_t)), "allocating samples buffer");
	}
	~SamplesSetup()
	{
		CUDA_ALERT(cudaFree(d_samples), "freeing samples buffer");
	}
public:
	int64_t * Samples()
	{
		return d_samples;
	}
};

// Used for allocating and auto-deallocating of the bytes buffer
// max_draws is the maximum number of sampling-tries per block
class BytesSamplerSetup
{
private:
	const size_t block_size, num_blocks, max_draws;
	unsigned int* d_data;
	curandGenerator_t gen;
public:
	BytesSamplerSetup(size_t block_size, size_t num_blocks, size_t max_draws) :
		block_size(block_size), num_blocks(num_blocks), max_draws(max_draws)
	{
		size_t s = sizeof(unsigned int);
		size_t n = ((block_size * num_blocks * max_draws) + s - 1) / s;
		CUDA_ALERT(cudaMalloc(&d_data, n * s), "allocating byte sampler buffer");
		CURAND_ALERT(curandCreateGenerator(&gen, CURAND_RNG_PSEUDO_DEFAULT), "creating bytes sampler PRG");
		CURAND_ALERT(curandSetPseudoRandomGeneratorSeed(gen, 1234ULL), "seeding bytes sampler PRG");
		CURAND_ALERT(curandGenerate(gen, d_data, n), "generating using bytes sampler PRG");
	}
	~BytesSamplerSetup()
	{
		CURAND_ALERT(curandDestroyGenerator(gen), "destroying bytes sampler PRG");
		CUDA_ALERT(cudaFree(d_data), "freeing bytes sampler buffer");
	}
public:
	unsigned char* Data()
	{
		return (unsigned char *)d_data;
	}
};

// Bytes sampler in CUDA mode
// max_draws is the maximum number of sampling-tries per block
class BytesSampler
{
private:
	unsigned char * d_data;
	size_t block_size, max_draws, draws;
public:
	__device__ BytesSampler(unsigned char * d_data, size_t block_size, size_t max_draws) :
		d_data(d_data), block_size(block_size), max_draws(max_draws), draws(0)
	{
	}
public:
	__device__ inline unsigned char * operator()(unsigned char* r, unsigned long long rlen)
	{
		if (draws >= max_draws) {
			// ran out of pre-generated bytes
			return nullptr;
		}
		++draws;
		unsigned char * result = d_data;
		d_data += block_size;
		return result;
	}
};

template<typename F>
float cuda_time_milliseconds(F f)
{
	float ms; // elapsed time in milliseconds
	cudaEvent_t startEvent, stopEvent;
	CUDA_ALERT(cudaEventCreate(&startEvent), "creating event");
	CUDA_ALERT(cudaEventCreate(&stopEvent), "creating event");
	CUDA_ALERT(cudaEventRecord(startEvent, 0), "recording event");
	f();
	CUDA_ALERT(cudaEventRecord(stopEvent, 0), "recording event");
	CUDA_ALERT(cudaEventSynchronize(stopEvent), "synchronizing event");
	CUDA_ALERT(cudaEventElapsedTime(&ms, startEvent, stopEvent), "measuring time");
	CUDA_ALERT(cudaEventDestroy(startEvent), "destroying event");
	CUDA_ALERT(cudaEventDestroy(stopEvent), "destroying event");
	return ms;
}

template<typename Facct>
__global__ void facct_sample_kernel(size_t block_size, size_t num_blocks, size_t max_draws, unsigned char* d_data, int64_t * d_samples)
{
	// assume block_size >= rlen in BytesSampler
	unsigned int tid = cuda_tid();
	if (tid < num_blocks) {
		// grab the pre-generated bytes for the current block
		BytesSampler bytes_sampler(d_data + block_size * max_draws * tid, block_size, max_draws);
		// set up the Facct sampler
		Facct facct(bytes_sampler);
		// generate up to FACCT_SAMPLE_BLOCK_SIZE samples and count their number
		uint32_t j = facct.sample(d_samples + FACCT_SAMPLE_BLOCK_SIZE * tid, FACCT_SAMPLE_BLOCK_SIZE);
		// mark any sample not generated as failed
		for (; j < FACCT_SAMPLE_BLOCK_SIZE; j++) {
			d_samples[FACCT_SAMPLE_BLOCK_SIZE * tid + j] = LLONG_MAX;
		}
	}
}

template<typename Facct>
void facct_sample(int64_t * samples, uint32_t num_blocks, size_t max_draws)
{
	// perpare bytes and sample buffers
	size_t block_size = Facct::ALL_SAMPLE_BLOCK_BYTES;
	BytesSamplerSetup bytes_sampler_setup(block_size, num_blocks, max_draws);
	SamplesSetup samples_setup(num_blocks);
	// prepare Facct-sample kernel
	unsigned int shift = 8, amount = 1 << shift;
	dim3 grid((num_blocks + amount - 1) >> shift), threads(1 << shift);
	auto f = [grid, threads, block_size, num_blocks, max_draws, &samples, &bytes_sampler_setup, &samples_setup]() {
		// launch Facct-sample kernel
		facct_sample_kernel<Facct> <<< grid, threads >>> (block_size, num_blocks, max_draws, bytes_sampler_setup.Data(), samples_setup.Samples());
		// sync and copy-back the Facct samples
		CUDA_ALERT(cudaDeviceSynchronize(), "synchronizing device");
		CUDA_ALERT(cudaMemcpy(samples, samples_setup.Samples(), num_blocks * FACCT_SAMPLE_BLOCK_SIZE * sizeof(int64_t), cudaMemcpyDeviceToHost), "copying facct samples to host");
	};
	// time the actual invocation on the device
	float ms = cuda_time_milliseconds(f);
	printf("Facct sampling kernel took %f milliseconds\n", ms);
}

template<uint32_t Sigma>
void facct_sample(size_t max_draws)
{
	// set up
	typedef BatchFacctGaussianSampler<BytesSampler, Sigma> Facct;
	uint32_t num_blocks = 1 << 20;
	int64_t* samples = (int64_t *)_aligned_malloc(num_blocks * FACCT_SAMPLE_BLOCK_SIZE * sizeof(int64_t), 64);
	// wall-clock time of Facct sampling
	std::chrono::high_resolution_clock::time_point t0 = std::chrono::high_resolution_clock::now();
	facct_sample<Facct>(samples, num_blocks, max_draws);
	std::chrono::high_resolution_clock::time_point t1 = std::chrono::high_resolution_clock::now();
	std::chrono::duration<double, std::milli> tdiff = t1 - t0;
	// estimate sample-failure rate
	int test_samples = 1000, failed_samples = 0;
	for (int i = 0; i < test_samples; i++) {
		if (samples[i] == LLONG_MAX) {
			failed_samples++;
		}
	}
	// report
	std::cerr << (num_blocks * FACCT_SAMPLE_BLOCK_SIZE) << " samples (" << (1.0 * (test_samples - failed_samples) / test_samples)
		<< " success rate) took " << tdiff.count() << " milliseconds." << std::endl;
	for (int i = 0; i < 1000; i++) {
		std::cerr << samples[i] << " ";
	}
	std::cerr << std::endl;
	// clean up
	_aligned_free(samples);
}

// make Facct sampling with sigma=215 available to non-CUDA code
void facct_sample_215()
{
	facct_sample<215>(2);
	CUDA_ALERT(cudaDeviceReset(), "resetting device");
}

// make Facct sampling with sigma=1M available to non-CUDA code
void facct_sample_1000000()
{
	facct_sample<1000000>(2);
	CUDA_ALERT(cudaDeviceReset(), "resetting device");
}

// make Facct sampling with sigma=2G available to non-CUDA code
void facct_sample_2000000000()
{
	facct_sample<2000000000>(8);
	CUDA_ALERT(cudaDeviceReset(), "resetting device");
}

	} // namespace Cuda
} // namespace LatticeZK
